{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "shakespeare.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd9tGHk26a3m",
        "outputId": "95a1b1c2-79d1-4a2a-de46-eac283b08453"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep 26 12:03:49 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpMHbaalepPf",
        "outputId": "161672f8-ab87-46e1-ec55-3fb3cbad799b"
      },
      "source": [
        "import torch\n",
        "torch.tril(torch.ones(1, 1, 10, 10))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "          [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "          [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "          [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "          [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "          [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "          [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "          [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXgTqkMALCts"
      },
      "source": [
        "##### GPT in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdZO82A-LCbM"
      },
      "source": [
        "import torch\n",
        "\n",
        "### Glossary\n",
        "# B = Batch size\n",
        "# D = Hidden dim\n",
        "# H = num Heads\n",
        "# S = Sequence length = block Size\n",
        "# L = Layers\n",
        "# V = Vocab size\n",
        "# E = Embedding dim\n",
        "# P = dropout Probability\n",
        "\n",
        "class SelfAttention(torch.nn.Module):\n",
        "    def __init__(self, D, H, S, P=0.2):\n",
        "        super().__init__()\n",
        "        if D % H != 0:\n",
        "            raise ValueError(\"Hidden Dim must be divisible by number of heads\")\n",
        "\n",
        "        self.H = H\n",
        "        self.DpH = D // H\n",
        "             \n",
        "        self.q_dense = torch.nn.Linear(D, D, bias=False)\n",
        "        self.k_dense = torch.nn.Linear(D, D, bias=False)\n",
        "        self.v_dense = torch.nn.Linear(D, D, bias=False)\n",
        "\n",
        "        self.drop = torch.nn.Dropout(P)\n",
        "\n",
        "        self.register_buffer(\"causal_mask\", torch.tril(torch.ones(1, 1, S, S)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, S, D = x.shape\n",
        "\n",
        "        q = self.q_dense(x).reshape([B, S, self.H, self.DpH]).transpose(1, 2) # Shape: [B, H, S, D/H]\n",
        "        k = self.k_dense(x).reshape([B, S, self.H, self.DpH]).permute(0, 2, 3, 1) # Shape: [B, H, D/H, S]\n",
        "        v = self.v_dense(x).reshape([B, S, self.H, self.DpH]).transpose(1, 2) # Shape: [B, H, S, D/H]\n",
        "\n",
        "        out = (q @ k) / (self.DpH ** (1/2)) # Shape: [B, H, S, S]\n",
        "\n",
        "        out = out.masked_fill(self.causal_mask[:, :, :S, :S] == 0, float('-inf')) # Shape: [B, H, S, S]\n",
        "        \n",
        "        out = torch.nn.functional.softmax(out, dim=-1) # Shape: [B, H, S, S]\n",
        "\n",
        "        out = self.drop(out) @ v # Shape: [B, H, S, D/H]\n",
        "\n",
        "        return out.transpose(1, 2).reshape([B, S, D])\n",
        "\n",
        "class GPTBlock(torch.nn.Module):\n",
        "    def __init__(self, D, H, S, P=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ln1 = torch.nn.LayerNorm(D)\n",
        "        self.ln2 = torch.nn.LayerNorm(D)\n",
        "        \n",
        "        self.att = SelfAttention(D, H, S, P)\n",
        "        self.ff = torch.nn.Sequential(\n",
        "            torch.nn.Linear(D, D*4), torch.nn.ReLU(), torch.nn.Linear(D*4, D), torch.nn.Dropout(P))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.att(self.ln1(x)) + x\n",
        "        x = self.ff(self.ln2(x)) + x\n",
        "        return x\n",
        "\n",
        "class GPT(torch.nn.Module):\n",
        "    def __init__(self, L=12, D=768, H=12, S=1024, V=50000, P=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embeddings\n",
        "        self.voc_emb = torch.nn.Embedding(V, D)\n",
        "        self.pos_emb = torch.nn.Embedding(S, D)\n",
        "        self.drop = torch.nn.Dropout(P)\n",
        "\n",
        "        # Transformer Blocks\n",
        "        self.blocks = torch.nn.Sequential(*[GPTBlock(D, H, S) for _ in range(L)])\n",
        "        self.ln3 = torch.nn.LayerNorm(D)\n",
        "\n",
        "        # Output\n",
        "        self.out = torch.nn.Linear(D, V, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "      \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0, std=0.02)\n",
        "            if isinstance(module, (torch.nn.Linear)) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, torch.nn.LayerNorm):\n",
        "            module.weight.data.fill_(1.0)\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        B, S = x.shape\n",
        "        pos_ids = torch.arange(S, dtype=torch.long, device=x.device)\n",
        "        x = self.voc_emb(x) + self.pos_emb(pos_ids)\n",
        "        x = self.blocks(self.drop(x))\n",
        "        x = self.out(self.ln3(x))\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = torch.nn.functional.cross_entropy(x.view(-1, x.size(-1)), targets.view(-1))\n",
        "        return x, loss"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JFDXhuW-Vqa",
        "outputId": "ba0acd8f-b9d0-4178-9bfc-d293e5c77f67"
      },
      "source": [
        "model = GPT()\n",
        "B, S = 6, 18 \n",
        "arr = torch.randint(0, 10, size=(B, S))\n",
        "model(arr)[0].shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 18, 50000])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j_wFuGX39tt"
      },
      "source": [
        "#Refs:\n",
        "#- https://github.com/huggingface/transformers/blob/master/src/transformers/models/gpt_neo/modeling_gpt_neo.py\n",
        "#- https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
        "#- https://github.com/huggingface/transformers/blob/master/src/transformers/models/gpt2/modeling_gpt2.py\n",
        "#- https://github.com/pbloem/former\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-VRRufp5bmq"
      },
      "source": [
        "##### Train & Generate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osMbRXtV47N7",
        "outputId": "10b0de24-ab48-4e79-d0bb-66fffc5a183c"
      },
      "source": [
        "# Use some boilerplate code for loading etc from a good repo\n",
        "!git clone https://github.com/karpathy/minGPT\n",
        "%cd minGPT"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'minGPT'...\n",
            "remote: Enumerating objects: 175, done.\u001b[K\n",
            "remote: Total 175 (delta 0), reused 0 (delta 0), pack-reused 175\u001b[K\n",
            "Receiving objects: 100% (175/175), 1.37 MiB | 8.06 MiB/s, done.\n",
            "Resolving deltas: 100% (101/101), done.\n",
            "/content/minGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-83JapxX4-c_"
      },
      "source": [
        "# set up logging\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        ")\n",
        "# make deterministic\n",
        "from mingpt.utils import set_seed\n",
        "set_seed(42)\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxPHJrV45Ue1"
      },
      "source": [
        "import math\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, block_size):\n",
        "        chars = sorted(list(set(data)))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "        \n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        # encode every character to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        \"\"\"\n",
        "        arrange data and targets so that the first i elements of x\n",
        "        will be asked to predict the i-th element of y. Notice that\n",
        "        the eventual language model will actually make block_size\n",
        "        individual predictions at the same time based on this data,\n",
        "        so we are being clever and amortizing the cost of the forward\n",
        "        pass of the network. So for example if block_size is 4, then\n",
        "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
        "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
        "        then actually \"multitask\" 4 separate examples at the same time\n",
        "        in the language model:\n",
        "        - given just \"h\", please predict \"e\" as next\n",
        "        - given \"he\" please predict \"l\" next\n",
        "        - given \"hel\" predict \"l\" next\n",
        "        - given \"hell\" predict \"o\" next\n",
        "        \n",
        "        In addition, because the DataLoader will create batches of examples,\n",
        "        every forward/backward pass during traning will simultaneously train\n",
        "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
        "        for a batched input of integers X (B, T) where B is batch size and\n",
        "        T is block_size and Y (B, T), the network will during training be\n",
        "        simultaneously training to make B*T predictions, all at once! Of course,\n",
        "        at test time we can paralellize across batch B, but unlike during training\n",
        "        we cannot parallelize across the time dimension T - we have to run\n",
        "        a forward pass of the network to recover the next single character of the \n",
        "        sequence along each batch dimension, and repeatedly always feed in a next\n",
        "        character to get the next one.\n",
        "        \n",
        "        So yes there is a big asymmetry between train/test time of autoregressive\n",
        "        models. During training we can go B*T at a time with every forward pass,\n",
        "        but during test time we can only go B at a time, T times, with T forward \n",
        "        passes.\n",
        "        \"\"\"\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxIeci5Q5hC_"
      },
      "source": [
        "block_size = 32 # spatial extent of the model for its context"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwyywvCg5vGp",
        "outputId": "8bb141e2-da32-4711-f32f-78bcdb228eb7"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
        "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-26 12:04:20--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-09-26 12:04:20 (50.0 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n",
            "data has 1115394 characters, 65 unique.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIoJ95jW5vlD"
      },
      "source": [
        "###### This GPT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5TKKq618Gia"
      },
      "source": [
        "model = GPT(V=train_dataset.vocab_size, S=train_dataset.block_size, L=6, H=8, D=256)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZFRMB8N96pM"
      },
      "source": [
        "# Add optimizer configuration to model to avoid having to rewrite Karpathy's training function\n",
        "def configure_optimizers(train_config):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=train_config.learning_rate, betas=train_config.betas)\n",
        "    return optimizer\n",
        "\n",
        "model.configure_optimizers = configure_optimizers"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBTlFY9aIZQa",
        "outputId": "284fba1e-4a1f-455c-d890-1f5f513a16a1"
      },
      "source": [
        "from mingpt.trainer import Trainer, TrainerConfig\n",
        "\n",
        "# initialize a trainer instance and kick off training\n",
        "tconf = TrainerConfig(max_epochs=2, batch_size=512, learning_rate=6e-4,\n",
        "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
        "                      num_workers=4)\n",
        "\n",
        "trainer = Trainer(model, train_dataset, None, tconf)\n",
        "trainer.train()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "epoch 1 iter 2178: train loss 1.37583. lr 3.000676e-04: 100%|██████████| 2179/2179 [05:15<00:00,  6.92it/s]\n",
            "epoch 2 iter 2178: train loss 1.27420. lr 6.000000e-05: 100%|██████████| 2179/2179 [05:14<00:00,  6.94it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe0OKFvn9SAF"
      },
      "source": [
        "def top_k_logits(logits, k):\n",
        "    v, ix = torch.topk(logits, k)\n",
        "    out = logits.clone()\n",
        "    out[out < v[:, [-1]]] = -float('Inf')\n",
        "    return out\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, x, steps, block_size, temperature=1.0, sample=False, top_k=None):\n",
        "    \"\"\"\n",
        "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
        "    the sequence, feeding the predictions back into the model each time. Clearly the sampling\n",
        "    has quadratic complexity unlike an RNN that is only linear, and has a finite context window\n",
        "    of block_size, unlike an RNN that has an infinite context window.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    for k in range(steps):\n",
        "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
        "        logits, _ = model(x_cond)\n",
        "        # pluck the logits at the final step and scale by temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # optionally crop probabilities to only the top k options\n",
        "        if top_k is not None:\n",
        "            logits = top_k_logits(logits, top_k)\n",
        "        # apply softmax to convert to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # sample from the distribution or take the most likely\n",
        "        if sample:\n",
        "            ix = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "        # append to the sequence and continue\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCN-uiZVE5VZ",
        "outputId": "5625cd35-2541-4e6c-f158-e207a0eedb20"
      },
      "source": [
        "# alright, let's sample some character-level Shakespeare\n",
        "\n",
        "context = \"O God, O God!\"\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "y = sample(model, x, 2000, block_size=train_dataset.block_size, temperature=1.0, sample=True, top_k=10)[0]\n",
        "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "print(completion)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O God, O God!\n",
            "O Pomfret,--\n",
            "\n",
            "Second Citizen:\n",
            "Ay, what a credit is strange? a gentleman of their holy hands\n",
            "Than an in place that she stands not a court\n",
            "Have been pawn'd upon you:'\n",
            "And I did not tell us alone, treasons that\n",
            "Is, here should welcome; yet, stir,\n",
            "To sure the seas of the west, but shall be this sweet was to crown to him?'\n",
            "\n",
            "SICINIUS:\n",
            "That's the noble credit of his cap,\n",
            "But they must lose upon your spirits,\n",
            "And trust me not to me too?\n",
            "\n",
            "CORIOLANUS:\n",
            "It is a fortune come to a southward to half the patricians that you are. Thou hast made the power\n",
            "Into bastards that bate thee.\n",
            "\n",
            "MERCUTIO:\n",
            "I know not, sir?\n",
            "\n",
            "POMPEY:\n",
            "Thou dost take thy life.\n",
            "\n",
            "KING EDWARD IV:\n",
            "To fear the city of our large: be gone.\n",
            "\n",
            "KATHARINA:\n",
            "It is the matter?\n",
            "\n",
            "GRUMIO:\n",
            "I am cowardly serve this shroud-favour!\n",
            "O, the mions, a dozen sir.\n",
            "\n",
            "GONZALO:\n",
            "Woe wisely faction of such best. Tribunes' sail,\n",
            "My short and sullen cease of the flame;\n",
            "And what thou attainted to have an old part,\n",
            "When she looks it not like a ten time: I cannot be\n",
            "good with your cause is my forbidden, as the morning of mine\n",
            "Whereby your father's blood whiles these frowns in this same shrewd.\n",
            "\n",
            "GLOUCESTER:\n",
            "What is my farther father, wherewithdraw\n",
            "The pates of your face to be her senseless power\n",
            "As to make us whose winds substance would\n",
            "not live to dear love.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Speed this is not a grace or a trouble.\n",
            "What? what's your good correct him?\n",
            "More haste: your work and free the top.\n",
            "What is the poor mayor must be construment.\n",
            "What sinest you must be said to leave him.\n",
            "But if you can have them. Force we met, till you have\n",
            "no wife-hallow'd her cunningly benefit at her both.\n",
            "\n",
            "GRUMIO:\n",
            "Ay, amen.\n",
            "\n",
            "KATHARINA:\n",
            "My lord, I'll plead thee from all.\n",
            "\n",
            "LUCIO:\n",
            "I see thine?\n",
            "\n",
            "PROSPERO:\n",
            "Who shall be so, by the same morning in the sea accusation: and he was\n",
            "I have; I'll bawl him them, if I such contract\n",
            "That which time the flatterers of my state,\n",
            "And not our part shall call. I see you will do not\n",
            "To find the wretch of his majesty.\n",
            "\n",
            "First Lord:\n",
            "We hope the charter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHNnV2ux5q4C"
      },
      "source": [
        "###### Karpathy's GPT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTB3uqC-5y47",
        "outputId": "ed24cd33-8095-4928-8b62-a1f3c5c7a7f4"
      },
      "source": [
        "from mingpt.model import GPT, GPTConfig\n",
        "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
        "                  n_layer=6, n_head=8, n_embd=256)\n",
        "model = GPT(mconf)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "09/25/2021 14:53:30 - INFO - mingpt.model -   number of parameters: 4.780544e+06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8cjwRih53Ij",
        "outputId": "469de932-6408-48e9-85b1-0ecb21bb5524"
      },
      "source": [
        "from mingpt.trainer import Trainer, TrainerConfig\n",
        "\n",
        "# initialize a trainer instance and kick off training\n",
        "tconf = TrainerConfig(max_epochs=1, batch_size=512, learning_rate=6e-4,\n",
        "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
        "                      num_workers=4)\n",
        "trainer = Trainer(model, train_dataset, None, tconf)\n",
        "trainer.train()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "epoch 1 iter 2178: train loss 1.31731. lr 3.000676e-04: 100%|██████████| 2179/2179 [08:17<00:00,  4.38it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO-En0wC57rK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df19378-db6d-4e2d-fd2e-0930184b3639"
      },
      "source": [
        "# alright, let's sample some character-level Shakespeare\n",
        "from mingpt.utils import sample\n",
        "\n",
        "context = \"O God, O God!\"\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
        "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "print(completion)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O God, O God! alas, bid her senators and my hearts\n",
            "Be hopeded on; for the sin of those faces.\n",
            "\n",
            "COMINIUS:\n",
            "Nay, sweet man; we have heard to be\n",
            "dream, an end all of his brother's wooing,\n",
            "And leave me a loss, to-morrow, be anon!\n",
            "\n",
            "CAPULET:\n",
            "If I did not be disposited; if he slept,\n",
            "For thoughts any minute in the heavens\n",
            "Had been a plant the faces them with och, would we disguise\n",
            "The dhich state shall do on you for a minute ask.\n",
            "\n",
            "GLOUCESTER:\n",
            "The park of lance, my good lord!\n",
            "\n",
            "GRUMIO:\n",
            "So wife, what's the matter?\n",
            "\n",
            "MENENIUS:\n",
            "And then here are no more.\n",
            "\n",
            "GREMIO:\n",
            "How didst thou bring my troth, soon-ear'd, and whose sanctuary begins honour.\n",
            "Think you for that, if you comfort.\n",
            "\n",
            "LUCIO:\n",
            "I have\n",
            "a damned friend?\n",
            "\n",
            "SAMPSON:\n",
            "That you must not prepettic son: I'll be consult,\n",
            "Her summiss of any cure will not conceive these senates and see\n",
            "That thinks you attaind the field; though they\n",
            "on our friautes part than you are: you\n",
            "will make it the legs, to cold young\n",
            "From the creature of your lady to this length.\n",
            "\n",
            "CLIFFORD:\n",
            "I'll tell thee what we seem: as thou dismiss it.\n",
            "\n",
            "SAMPSON:\n",
            "Well, to walk your grossip out wonder with a pictures\n",
            "That endure them that in the common loyal pleasure to thee,\n",
            "Our he whon they had present it no more,\n",
            "Than interpossible port.\n",
            "\n",
            "LADY GREY:\n",
            "That is your father and death, and toward,\n",
            "As fellow a blind as much doth a summon-left stragon, as he cannot name to the son;\n",
            "He was my song-stance. The woeful lady was our flight with his drunkingn by love?\n",
            "O, that hast thou not the high, here's seven.\n",
            "\n",
            "CURTIS:\n",
            "She will not dirkly, but yield our croan;\n",
            "Where have a mine adverse may be myself alone,--\n",
            "And your honesty and still be hearing to add;\n",
            "What witness is all strike, that's the man of the cels!\n",
            "\n",
            "LUCIO:\n",
            "O, what, I say; he'll be calmity.\n",
            "\n",
            "Pedant:\n",
            "What, as I mount? What is this? how hast thou, that\n",
            "way to be drunk for the heaving of the people,\n",
            "No more same state, sir, and behind me!\n",
            "\n",
            "DUHESS OF YORK:\n",
            "A counsel, i the mind of heaven?\n",
            "\n",
            "AUTOLYCUS:\n",
            "My lord,\n",
            "Thou mayst nature, but we can be absen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3T4hOH9PovE"
      },
      "source": [
        "##### Micro"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AVYnPCyD4M5",
        "outputId": "91cc69cf-2ab5-4f51-e6eb-cc83544a33dc"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-26 12:23:35--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-09-26 12:23:35 (31.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htM9lZX4Pp4G"
      },
      "source": [
        "class CharDataset:\n",
        "\n",
        "    def __init__(self, data, block_size):\n",
        "        chars = sorted(list(set(data)))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "        \n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        # encode every character to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        " \n",
        "        x = dix[:-1]\n",
        "        y = dix[1:]\n",
        "        return x, y\n",
        "\n",
        "block_size = 32 # spatial extent of the model for its context"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtH0-DfLQtTM",
        "outputId": "dd942924-19a4-4d71-b391-b6bb33ba1ce8"
      },
      "source": [
        "import random\n",
        "\n",
        "block_size = 12\n",
        "train_dataset = CharDataset(text, block_size)\n",
        "X, y = train_dataset[random.randint(0, len(train_dataset))]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 1115394 characters, 65 unique.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KysPgjmaRtM1"
      },
      "source": [
        "import math\n",
        "\n",
        "class Value:\n",
        "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
        "\n",
        "    def __init__(self, data, _children=(), _op=''):\n",
        "        self.data = data\n",
        "        self.grad = 0\n",
        "        # internal variables used for autograd graph construction\n",
        "        self._backward = lambda: None\n",
        "        self._prev = set(_children)\n",
        "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += out.grad\n",
        "            other.grad += out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "    \n",
        "    ### Added from a pr ###\n",
        "    def exp(self):\n",
        "        try:\n",
        "            out = Value(math.exp(self.data), (self,), \"e\")\n",
        "        except:\n",
        "            # This happens when e.g. not using Layernorm\n",
        "            print(\"Data too large: \", self.data)\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += math.exp(self.data) * out.grad\n",
        "\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    ### Added ###\n",
        "    def log(self):\n",
        "\n",
        "        out = Value(math.log(self.data), (self,), \"ln\")\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (1 / self.data) * out.grad\n",
        "        \n",
        "        out._backward = _backward\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += other.data * out.grad\n",
        "            other.grad += self.data * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
        "        out = Value(self.data**other, (self,), f'**{other}')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (other * self.data**(other-1)) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def relu(self):\n",
        "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (out.data > 0) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self):\n",
        "\n",
        "        # topological order all of the children in the graph\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "\n",
        "        # go one variable at a time and apply the chain rule to get its gradient\n",
        "        self.grad = 1\n",
        "        for v in reversed(topo):\n",
        "            v._backward()\n",
        "\n",
        "    def __neg__(self): # -self\n",
        "        return self * -1\n",
        "\n",
        "    def __radd__(self, other): # other + self\n",
        "        return self + other\n",
        "\n",
        "    def __sub__(self, other): # self - other\n",
        "        return self + (-other)\n",
        "\n",
        "    def __rsub__(self, other): # other - self\n",
        "        return other + (-self)\n",
        "\n",
        "    def __rmul__(self, other): # other * self\n",
        "        return self * other\n",
        "\n",
        "    def __truediv__(self, other): # self / other\n",
        "        return self * other**-1\n",
        "\n",
        "    def __rtruediv__(self, other): # other / self\n",
        "        return other * self**-1\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Value(data={self.data}, grad={self.grad})\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp7HpX1eRphs"
      },
      "source": [
        "import random\n",
        "\n",
        "class Module:\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for p in self.parameters():\n",
        "            p.grad = 0\n",
        "\n",
        "    def parameters(self):\n",
        "        return []\n",
        "\n",
        "class Neuron(Module):\n",
        "\n",
        "    def __init__(self, nin, nonlin=True):\n",
        "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
        "        self.b = Value(0)\n",
        "        self.nonlin = nonlin\n",
        "\n",
        "    def __call__(self, x):\n",
        "        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
        "        return act.relu() if self.nonlin else act\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.w + [self.b]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n",
        "\n",
        "class Layer(Module):\n",
        "\n",
        "    def __init__(self, nin, nout, **kwargs):\n",
        "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        out = [n(x) for n in self.neurons]\n",
        "        return out[0] if len(out) == 1 else out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [p for n in self.neurons for p in n.parameters()]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
        "\n",
        "class MLP(Module):\n",
        "\n",
        "    def __init__(self, nin, nouts):\n",
        "        sz = [nin] + nouts\n",
        "        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def parameters(self):\n",
        "        return [p for layer in self.layers for p in layer.parameters()]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ8kU4Dg4wsE",
        "outputId": "7e27e50d-ddd7-4d43-8aa0-4700c9d38eed"
      },
      "source": [
        "def layernorm(x, eps=1e-5):\n",
        "    \"\"\"Calculates simplified layernorm over rows of 2-dim array\"\"\"\n",
        "    # https://stackoverflow.com/questions/59830168/layer-normalization-in-pytorch\n",
        "    # https://github.com/geohot/tinygrad/blob/master/models/transformer.py\n",
        "\n",
        "    means = [sum(row) / len(row) for row in x]\n",
        "\n",
        "    # Calculate variance\n",
        "    vars = []\n",
        "    for i in range(len(x)):\n",
        "        mean_diffs = []\n",
        "        for j in range(len(x[i])):\n",
        "            mean_diffs.append((x[i][j] - means[i]) ** 2)\n",
        "        vars.append(sum(mean_diffs) / len(mean_diffs))\n",
        "\n",
        "    for i in range(len(x)):\n",
        "        for j in range(len(x[i])):\n",
        "            x[i][j] = (x[i][j] - means[i]) / ((vars[i] + eps) ** (1/2))\n",
        "    print(means)\n",
        "    print(vars)\n",
        "    return x\n",
        "\n",
        "layernorm([[1,2], [1,2]])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.5, 1.5]\n",
            "[0.25, 0.25]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[-0.9999800005999799, 0.9999800005999799],\n",
              " [-0.9999800005999799, 0.9999800005999799]]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DwEG4ZeRnAq"
      },
      "source": [
        "import random\n",
        "\n",
        "def dot(v1, v2):\n",
        "    \"\"\"Vector dot product of two vectors with same length\"\"\"\n",
        "    return sum([x*y for x,y in zip(v1, v2)])\n",
        "\n",
        "def matmul(A, B):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      A: Matrix with shape (X, Y) = X rows of length Y\n",
        "      B: Matrix with shape (Y, Z) = Y rows of length Z = Z cols of length Y\n",
        "\n",
        "    Returns:\n",
        "      Matrix with shape (X, Z)\n",
        "    \"\"\"\n",
        "    # Invert B to be (Z, Y)\n",
        "    B = list(zip(*B))\n",
        "    out = []\n",
        "    for row_a in A:\n",
        "        new_row = []\n",
        "        for col_b in B:\n",
        "            # row of length Y & col of length Y\n",
        "            assert len(row_a) == len(col_b)\n",
        "            new_row.append(dot(row_a, col_b))\n",
        "        out.append(new_row)\n",
        "    return out\n",
        "\n",
        "def layernorm(x, eps=1e-5):\n",
        "    \"\"\"Calculates simplified layernorm over rows of 2-dim array\"\"\"\n",
        "\n",
        "    means = [sum(row) / len(row) for row in x]\n",
        "\n",
        "    # Calculate variance\n",
        "    vars = []\n",
        "    for i in range(len(x)):\n",
        "        mean_diffs = []\n",
        "        for j in range(len(x[i])):\n",
        "            mean_diffs.append((x[i][j] - means[i]) ** 2)\n",
        "        vars.append(sum(mean_diffs) / len(mean_diffs))\n",
        "\n",
        "    for i in range(len(x)):\n",
        "        for j in range(len(x[i])):\n",
        "            x[i][j] = (x[i][j] - means[i]) / ((vars[i] + eps) ** (1/2))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def softmax(x, dim=1):\n",
        "    \"\"\"Applies softmax to 2-dimensional array - This can be done better\"\"\"\n",
        "    if dim == 0:\n",
        "        x = list(zip(*x))\n",
        "    \n",
        "    out = []\n",
        "    for i in range(0, len(x)):\n",
        "        out.append([x[i][j].exp() for j in range(0, len(x[i]))])\n",
        "\n",
        "    for i in range(0, len(x)):\n",
        "        denom = sum(out[i])\n",
        "        for j in range(len(out[i])):\n",
        "            out[i][j] /= denom\n",
        "\n",
        "    if dim == 0:\n",
        "        return list(zip(*out))\n",
        "    return out\n",
        "\n",
        "class SelfAttention(Module):\n",
        "    def __init__(self, D, S):\n",
        "        super().__init__()\n",
        "\n",
        "        # Micrograd doesn't give an option to turn off bias\n",
        "        self.q_dense = Layer(D, D)\n",
        "        self.k_dense = Layer(D, D)\n",
        "        self.v_dense = Layer(D, D)\n",
        "\n",
        "        # Micrograd has no dropout\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \n",
        "        # No batch size \n",
        "        S = len(x)\n",
        "        D = len(x[0])\n",
        "\n",
        "        # Apply the same linear weights for each seq length unit\n",
        "        q = list(map(self.q_dense, x)) # Shape: [S, D]\n",
        "        k = list(map(self.k_dense, x)) # Shape: [S, D]\n",
        "        v = list(map(self.v_dense, x)) # Shape: [S, D]\n",
        "\n",
        "        out = matmul(q, list(zip(*k))) # Shape: [S, S]\n",
        "\n",
        "        # Masking of future values\n",
        "        for i in range(0, S):\n",
        "            for j in range(i+1, S):\n",
        "                # Hide the ith row * jth column\n",
        "                out[i][j].data = -9999 # using float('-inf') introduces NaNs\n",
        "\n",
        "        # Apply softmax across columns of each row\n",
        "        out = softmax(out, dim=1) # Shape: [S, S]\n",
        "\n",
        "        out = matmul(out, v) # Shape: [S, D]\n",
        "\n",
        "        return out\n",
        "\n",
        "    def parameters(self):\n",
        "        q_pars = [p for p in self.q_dense.parameters()]\n",
        "        k_pars = [p for p in self.k_dense.parameters()]\n",
        "        v_pars = [p for p in self.v_dense.parameters()]\n",
        "        return q_pars + k_pars + v_pars\n",
        "\n",
        "\n",
        "class GPTBlock(Module):\n",
        "    def __init__(self, D, S):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.att = SelfAttention(D, S)\n",
        "        \n",
        "        # MLP without dropout\n",
        "        self.ff = MLP(D, [D*4, D*4, D])\n",
        "\n",
        "    def __call__(self, x):\n",
        "\n",
        "        att_out = self.att(layernorm(x))\n",
        "\n",
        "        # Add Residual\n",
        "        for i in range(len(att_out)):\n",
        "            for j in range(len(att_out[i])):\n",
        "                att_out[i][j] += x[i][j]\n",
        "\n",
        "        ff_out = list(map(self.ff, layernorm(att_out)))\n",
        "\n",
        "        # Add Residual\n",
        "        for i in range(len(ff_out)):\n",
        "            for j in range(len(ff_out[i])):\n",
        "                ff_out[i][j] += att_out[i][j]\n",
        "\n",
        "        return ff_out\n",
        "\n",
        "    def parameters(self):\n",
        "        att_pars = self.att.parameters()\n",
        "        ff_pars = self.ff.parameters()\n",
        "        return att_pars + ff_pars\n",
        "\n",
        "\n",
        "class GPT(Module):\n",
        "    def __init__(self, L=2, D=8, S=12, V=26, P=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embeddings\n",
        "        self.voc_emb = [[Value(random.uniform(-1,1)) for _ in range(D)] for _ in range(V)]\n",
        "        self.pos_emb = [[Value(random.uniform(-1,1)) for _ in range(D)] for _ in range(S)]\n",
        "\n",
        "        # Transformer Blocks\n",
        "        self.blocks = [GPTBlock(D, S) for _ in range(L)]\n",
        "\n",
        "        # Output\n",
        "        self.out = Layer(D, V)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "          Args:\n",
        "            x: List of length S\n",
        "          Returns:\n",
        "            y: List of length S\n",
        "        \"\"\"\n",
        "        voc_emb = [self.voc_emb[idx] for idx in x] # Shape [S, D]\n",
        "        pos_emb = [self.pos_emb[idx] for idx in range(len(x))] # Shape [S, D]\n",
        "\n",
        "        x = [[sum(z) for z in zip(sub_v, sub_p)] for sub_v, sub_p in zip(voc_emb,pos_emb)]\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        return list(map(self.out, layernorm(x)))\n",
        "\n",
        "    def parameters(self):\n",
        "        emb_pars = [p for p_list in self.voc_emb for p in p_list] + [p for p_list in self.pos_emb for p in p_list]\n",
        "        block_pars = [p for block in self.blocks for p in block.parameters()]\n",
        "        out_pars = self.out.parameters()\n",
        "        return block_pars + out_pars + emb_pars\n",
        "      "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aimrp3yJ-qxf"
      },
      "source": [
        "model = GPT(V=train_dataset.vocab_size, S=train_dataset.block_size, L=4, D=8)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX5wvjcxRy8h",
        "outputId": "e9ff9339-c3d8-4e2f-cc6d-f1c77927a340"
      },
      "source": [
        "model(X)[:1][:1]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Value(data=2.4507031380821003, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0.9412224612929179, grad=0),\n",
              "  Value(data=1.5766944155716995, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=3.0164231674399127, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0.3762352462049908, grad=0),\n",
              "  Value(data=0.3816561060676874, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=1.0081037939290172, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0.07226779134780155, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=1.5124330201432963, grad=0),\n",
              "  Value(data=0.26858885207170785, grad=0),\n",
              "  Value(data=0.9976278674812337, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=2.1927261355384986, grad=0),\n",
              "  Value(data=0.025301942461674873, grad=0),\n",
              "  Value(data=2.9928753642383756, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0.7966334557368777, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=1.130934549454099, grad=0),\n",
              "  Value(data=1.171331380567658, grad=0),\n",
              "  Value(data=0.3902229441551338, grad=0),\n",
              "  Value(data=2.325872745162822, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0.6608874679350308, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=2.5433911801644227, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=1.9901061089402696, grad=0),\n",
              "  Value(data=1.2946476413866115, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=1.1277329637297513, grad=0),\n",
              "  Value(data=0.6421286447486273, grad=0),\n",
              "  Value(data=0.014586414538014576, grad=0),\n",
              "  Value(data=1.2008218198691551, grad=0),\n",
              "  Value(data=0.6085302819485843, grad=0),\n",
              "  Value(data=0, grad=0),\n",
              "  Value(data=0.9308733325078334, grad=0),\n",
              "  Value(data=0, grad=0)]]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MavmJUs_P3LF",
        "outputId": "6131092c-53a0-4de8-fb99-991a7b27e31a"
      },
      "source": [
        "# loss function\n",
        "# great ref: https://github.com/karpathy/micrograd/blob/master/demo.ipynb\n",
        "def loss(batch_size=8):\n",
        "\n",
        "    def cross_entropy(preds, targets, eps=1e-15):\n",
        "        losses = []\n",
        "        for i, tar in enumerate(targets):\n",
        "            stab = preds[i][tar] + eps\n",
        "            losses.append(stab.log())\n",
        "        return sum(losses) * -1\n",
        "\n",
        "    losses = []\n",
        "    for _ in range(batch_size):\n",
        "        X, y = dataset[random.randint(0, len(dataset))]\n",
        "\n",
        "        # forward the model to get scores\n",
        "        scores = model(X)\n",
        "\n",
        "        loss = cross_entropy(scores, y)\n",
        "\n",
        "\n",
        "        losses.append(loss)\n",
        "\n",
        "    total_loss = sum(losses) * (1.0 / len(losses))\n",
        "    return total_loss\n",
        "\n",
        "total_loss = loss()\n",
        "print(total_loss)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value(data=243.22317495606308, grad=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUzsY9ZPf2we",
        "outputId": "bc82a56a-c7a5-4e40-aaca-1cd0d63c8efc"
      },
      "source": [
        "# optimization\n",
        "for k in range(100):\n",
        "    \n",
        "    # forward\n",
        "    total_loss = loss()\n",
        "    \n",
        "    # backward\n",
        "    model.zero_grad()\n",
        "    total_loss.backward()\n",
        "    \n",
        "    # update (sgd)\n",
        "    learning_rate = 1.0 - 0.9*k/100\n",
        "    for p in model.parameters():\n",
        "        p.data -= learning_rate * p.grad\n",
        "    \n",
        "    if k % 1 == 0:\n",
        "        print(f\"step {k} loss {total_loss.data}\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0 loss 260.388675723954\n",
            "step 1 loss 179.66027812337185\n",
            "step 2 loss 152.23192022407562\n",
            "step 3 loss 148.35509570659246\n",
            "step 4 loss 141.22000951388566\n",
            "step 5 loss 138.42312501308777\n",
            "step 6 loss 158.22589162778664\n",
            "step 7 loss 154.41517576856418\n",
            "step 8 loss 121.92370253488497\n",
            "step 9 loss 143.33694976879545\n",
            "step 10 loss 185.01080258767988\n",
            "step 11 loss 195.92509034067305\n",
            "step 12 loss 147.3328983898601\n",
            "step 13 loss 135.0645504490134\n",
            "step 14 loss 147.68345167692647\n",
            "step 15 loss 134.84393229909762\n",
            "step 16 loss 131.28019703527013\n",
            "step 17 loss 113.74112237891173\n",
            "step 18 loss 161.54996543036418\n",
            "step 19 loss 151.45381082075147\n",
            "step 20 loss 150.08157526552412\n",
            "step 21 loss 181.2657241668948\n",
            "step 22 loss 142.22579784912662\n",
            "step 23 loss 157.43751061068892\n",
            "step 24 loss 160.42373408671995\n",
            "step 25 loss 126.60887299960415\n",
            "step 26 loss 168.74995921866125\n",
            "step 27 loss 126.20308947476586\n",
            "step 28 loss 158.92740147056662\n",
            "step 29 loss 141.1868595990307\n",
            "step 30 loss 145.3663107159982\n",
            "step 31 loss 140.25698596033615\n",
            "step 32 loss 129.49802159791165\n",
            "step 33 loss 164.40744633227243\n",
            "step 34 loss 179.28079337428596\n",
            "step 35 loss 110.54501466599206\n",
            "step 36 loss 136.7266080139608\n",
            "step 37 loss 117.26832659361982\n",
            "step 38 loss 159.1765182172592\n",
            "step 39 loss 139.22167413947255\n",
            "step 40 loss 109.71204497413984\n",
            "step 41 loss 141.906925404142\n",
            "step 42 loss 133.21570321447376\n",
            "step 43 loss 138.51899136395258\n",
            "step 44 loss 151.33798723327845\n",
            "step 45 loss 149.11240114431843\n",
            "step 46 loss 129.67579689559744\n",
            "step 47 loss 108.2323617674963\n",
            "step 48 loss 155.3824486359574\n",
            "step 49 loss 120.20694088819819\n",
            "step 50 loss 148.28961374995677\n",
            "step 51 loss 149.4342894481802\n",
            "step 52 loss 126.16700464108625\n",
            "step 53 loss 158.10943662061646\n",
            "step 54 loss 149.29131764406594\n",
            "step 55 loss 111.51030639020372\n",
            "step 56 loss 174.5858231849421\n",
            "step 57 loss 154.4823016661048\n",
            "step 58 loss 117.65832629364486\n",
            "step 59 loss 147.6017094285083\n",
            "step 60 loss 123.27697535043343\n",
            "step 61 loss 118.61730994970762\n",
            "step 62 loss 168.78046078423293\n",
            "step 63 loss 125.42512999313364\n",
            "step 64 loss 125.36180207004868\n",
            "step 65 loss 161.10449096928136\n",
            "step 66 loss 134.22775016587312\n",
            "step 67 loss 141.74409562523283\n",
            "step 68 loss 147.40253165126526\n",
            "step 69 loss 178.9344461763922\n",
            "step 70 loss 160.22857484541382\n",
            "step 71 loss 154.0934354998328\n",
            "step 72 loss 191.9382228899111\n",
            "step 73 loss 146.76366384609486\n",
            "step 74 loss 151.38871908380077\n",
            "step 75 loss 119.81631769440514\n",
            "step 76 loss 148.13317961902436\n",
            "step 77 loss 125.53500711933043\n",
            "step 78 loss 160.21397260633307\n",
            "step 79 loss 126.35629563575398\n",
            "step 80 loss 157.42336927644757\n",
            "step 81 loss 130.10350360989418\n",
            "step 82 loss 120.43105283175242\n",
            "step 83 loss 143.30788728663953\n",
            "step 84 loss 160.00185795365718\n",
            "step 85 loss 157.91279539929317\n",
            "step 86 loss 152.48150613521958\n",
            "step 87 loss 154.41770410890183\n",
            "step 88 loss 163.6232042773624\n",
            "step 89 loss 154.58131467543942\n",
            "step 90 loss 151.62156326261385\n",
            "step 91 loss 141.2458519954522\n",
            "step 92 loss 171.7306521339007\n",
            "step 93 loss 134.6406822641427\n",
            "step 94 loss 111.07639837301818\n",
            "step 95 loss 158.9219706155804\n",
            "step 96 loss 100.06809540029944\n",
            "step 97 loss 153.0467088655577\n",
            "step 98 loss 131.95481146190193\n",
            "step 99 loss 150.10279698787878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3Xru1qxy4cR",
        "outputId": "2c63c14f-4b6a-48f6-e503-200de9f7baaf"
      },
      "source": [
        "# alright, let's sample some character-level Shakespeare\n",
        "\n",
        "context = \"O God, O\"\n",
        "x = [train_dataset.stoi[s] for s in context]\n",
        "\n",
        "\n",
        "model.zero_grad()\n",
        "\n",
        "out_seq = \"\"\n",
        "\n",
        "for k in range(5):\n",
        "    logits = model(x) # Shape [S, V]\n",
        "    # Pick the final sequence one, i.e. whats predicted after the last char\n",
        "    data = [x.data for x in logits[-1]]\n",
        "\n",
        "    pred_idx = data.index(max(data))\n",
        "    pred_char = train_dataset.itos[pred_idx]\n",
        "\n",
        "    x.append(pred_idx)\n",
        "    out_seq += pred_char\n",
        "\n",
        "# Well maybe this was a bit overkill - there might be bugs in the model & the model is tiny..\n",
        "print(context)\n",
        "print(out_seq)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O God, O\n",
            "eeeee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5opZxKe1NLN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}